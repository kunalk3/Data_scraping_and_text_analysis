{"cells":[{"metadata":{},"cell_type":"markdown","source":"With the Python programming language, you have a myriad of options to use in order to remove stop words from strings. You can either use one of the several natural language processing libraries such as **NLTK, SpaCy, Gensim, TextBlob**, etc., or if you need full control on the stop words that you want to remove, you can write your own custom script.\n\nHere in this current notebook, you will see a number of different the approaches, depending on the NLP library you're using.\n\n**A) Stop Words with NLTK**\n\n**B) Stop Words with Gensim**\n\n**C) Stop Words with SpaCy**\n\n**D) Custom Script to Remove Stop Words**"},{"metadata":{},"cell_type":"markdown","source":"## A) Method 1: Using Python's NLTK Library\n\nThe NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\n\n*Let's see a below code:*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import library\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\n\n# text data\ntext = 'This is a sample sentence, showing off the stop words filtration.'\n\n# tokenized\ntext_tokens = word_tokenize(text)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords = [word for word in text_tokens if not word in stopwords.words()]\nprint('Tokens without stopwords : ', tokens_without_stopwords)\n\nprint('\\nOriginal text : ', text)\nfiltered_sentence = (\" \").join(tokens_without_stopwords)\nprint('Filtered text : ', filtered_sentence)","execution_count":1,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\nTokens in text :  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\nTokens without stopwords :  ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n\nOriginal text :  This is a sample sentence, showing off the stop words filtration.\nFiltered text :  This sample sentence , showing stop words filtration .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"In the above code, we first import the **stopwords** collection from the **nltk.corpus** module. Next, we import the **word_tokenize()** method from the nltk.tokenize class. We then create a variable text, which contains a simple sentence. The sentence in the text variable is tokenized (divided into words) using the word_tokenize() method. Next, we iterate through all the words in the text_tokens list and checks if the word exists in the stop words collection or not. If the word doesn't exist in the stopword collection, it is returned and appended to the **tokens_without_sw** list. The tokens_without_sw list is then printed.\n\n- You can see that the words **is, a, off,** and **the** have been removed from the sentence."},{"metadata":{},"cell_type":"markdown","source":"### A-1) Adding or Removing Stop Words in NLTK's Default Stop Word List\nYou can add or remove stop words as per your choice to the existing collection of stop words in NLTK. Before removing or adding stop words in NLTK, let's see the list of all the English stop words supported by NLTK\n\n**A-1-a) Adding Stop Words to Default NLTK Stop Word List**\n\n**A-1-b) Removing Stop Words from Default NLTK Stop Word List**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of stop words in English language\nprint(stopwords.words('english'))","execution_count":2,"outputs":[{"output_type":"stream","text":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### A-1-a) Adding Stop Words to Default NLTK Stop Word List\nTo add a word to NLTK stop words collection, first create an object from the **stopwords.words('english')** list. Next, use the **append()** method on the list to add any word to the list.\n\nThe following script adds the word play to the NLTK stop word collection. Again, we remove all the words from our text variable to see if the word **sample** is removed or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_stopwords = stopwords.words('english')\n\n# added our own stopword \"sample\" in list of pre-defined stopwords\nall_stopwords.append('sample')\n\n# tokenized\ntext_tokens = word_tokenize(text)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords = [word for word in text_tokens if not word in all_stopwords]\nprint('Tokens without stopwords : ', tokens_without_stopwords)\n\nprint('\\nOriginal text : ', text)\nfiltered_sentence = (\" \").join(tokens_without_stopwords)\nprint('Filtered text : ', filtered_sentence)","execution_count":3,"outputs":[{"output_type":"stream","text":"\nTokens in text :  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\nTokens without stopwords :  ['This', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n\nOriginal text :  This is a sample sentence, showing off the stop words filtration.\nFiltered text :  This sentence , showing stop words filtration .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- The output shows that the word **sample** has been removed.\n\nYou can also add a list of words to the stopwords.words list using the append method, as shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# added our own stopwords in list of pre-defined stopwords\nsw_list = ['Kunal', 'Kolhe', 'article']\nall_stopwords.extend(sw_list)\n\n# text data\ntext_data = 'This is how we are making our processed content more efficient by removing words that do \\\nnot contribute to any future operations. This article is contributed by Kunal Kolhe.'\n\n# tokenized\ntext_tokens = word_tokenize(text_data)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords = [word for word in text_tokens if not word in all_stopwords]\nprint('Tokens without stopwords : ', tokens_without_stopwords)\n\nprint('\\nOriginal text : ', text_data)\nfiltered_sentence = (\" \").join(tokens_without_stopwords)\nprint('Filtered text : ', filtered_sentence)","execution_count":4,"outputs":[{"output_type":"stream","text":"\nTokens in text :  ['This', 'is', 'how', 'we', 'are', 'making', 'our', 'processed', 'content', 'more', 'efficient', 'by', 'removing', 'words', 'that', 'do', 'not', 'contribute', 'to', 'any', 'future', 'operations', '.', 'This', 'article', 'is', 'contributed', 'by', 'Kunal', 'Kolhe', '.']\nTokens without stopwords :  ['This', 'making', 'processed', 'content', 'efficient', 'removing', 'words', 'contribute', 'future', 'operations', '.', 'This', 'contributed', '.']\n\nOriginal text :  This is how we are making our processed content more efficient by removing words that do not contribute to any future operations. This article is contributed by Kunal Kolhe.\nFiltered text :  This making processed content efficient removing words contribute future operations . This contributed .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- The script above adds two words **Kunal, Kolhe** and **article** to the stopwords.word"},{"metadata":{},"cell_type":"markdown","source":"### A-1-b) Removing Stop Words from Default NLTK Stop Word List\nSince stopwords.word('english') is merely a list of items, you can remove items from this list like any other list. The simplest way to do so is via the **remove()** method. This is helpful for when your application needs a stop word to not be removed. For example, you may need to keep the word not in a sentence to know when a statement is being negated.\n- The following script removes the stop word **not** from the default list of stop words in NLTK:"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_stopwords = stopwords.words('english')\n\n# added our own stopword \"off\"\nall_stopwords.remove('off')\n\n# tokenized\ntext_tokens = word_tokenize(text)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords = [word for word in text_tokens if not word in all_stopwords]\nprint('Tokens without stopwords : ', tokens_without_stopwords)\n\nprint('\\nOriginal text : ', text)\nfiltered_sentence = (\" \").join(tokens_without_stopwords)\nprint('Filtered text : ', filtered_sentence)","execution_count":5,"outputs":[{"output_type":"stream","text":"\nTokens in text :  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\nTokens without stopwords :  ['This', 'sample', 'sentence', ',', 'showing', 'off', 'stop', 'words', 'filtration', '.']\n\nOriginal text :  This is a sample sentence, showing off the stop words filtration.\nFiltered text :  This sample sentence , showing off stop words filtration .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- From the output, you can see that the word **off** has not been removed from the input sentence."},{"metadata":{},"cell_type":"markdown","source":"## B) Method 2: Using Python's Gensim Library\nThe Gensim library is another extremely useful library for removing stop words from a string in Python. All you have to do is to import the remove_stopwords() method from the **gensim.parsing.preprocessin**g module. Next, you need to pass your sentence from which you want to remove stop words, to the **remove_stopwords()** method which returns text string without the stop words.\n\n*Let's see a below code:*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nfrom gensim.parsing.preprocessing import remove_stopwords\n\n# text data\ntext = 'This is a sample sentence, showing off the stop words filtration.'\n\n# applied stop words on text\nfiltered_sentence = remove_stopwords(text)\n\nprint('\\nOriginal text : ', text)\nprint('Filtered text : ', filtered_sentence)","execution_count":6,"outputs":[{"output_type":"stream","text":"\nOriginal text :  This is a sample sentence, showing off the stop words filtration.\nFiltered text :  This sample sentence, showing stop words filtration.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"It is important to mention that the output after removing stop words using the NLTK and Gensim libraries is different. For example, the Gensim library considered the word however to be a stop word while NLTK did not, and hence didn't remove it. This shows that there is no hard and fast rule as to what a stop word is and what it isn't. It all depends upon the task that you are going to perform."},{"metadata":{},"cell_type":"markdown","source":"### B-1) Adding and Removing Stop Words in Default Gensim Stop Words List\nYou can add or remove stop words as per your choice to the existing collection of stop words in Gensim. Before removing or adding stop words in Gensim, let's see the list of all the English stop words supported by Gnsim.\n\n**B-1-a) Adding Stop Words to Default Gensim Stop Words List**\n\n**B-1-b) Removing Stop Words from Default Gensim Stopword List**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\n# list of stop words in English language\nall_stopwords = gensim.parsing.preprocessing.STOPWORDS\nprint(all_stopwords)","execution_count":7,"outputs":[{"output_type":"stream","text":"frozenset({'into', 'doing', 'too', 'most', 'less', 'call', 'least', 'who', 'twelve', 'part', 'whence', 'what', 'yourselves', 'hence', 'in', 'and', 'amount', 'eleven', 'by', 'how', 'per', 'yourself', 'never', 'many', 'fill', 'couldnt', 'neither', 'formerly', 'hundred', 'however', 'herself', 'anything', 'km', 'name', 'ltd', 'your', 'anywhere', 'anyone', 'others', 'their', 'three', 'such', 'thence', 'them', 'ie', 'unless', 'several', 'whoever', 'behind', 'whatever', 'don', 'still', 'seemed', 'would', 'only', 'does', 'more', 'any', 'thick', 'hasnt', 'made', 'further', 'mine', 'empty', 'inc', 'give', 'also', 'everything', 'hereafter', 'upon', 'back', 'except', 'across', 'whereupon', 'always', 'quite', 'now', 'after', 'nor', 'those', 'either', 'will', 'noone', 'thereafter', 'anyhow', 'himself', 'an', 'myself', 'although', 'these', 'nevertheless', 'that', 'my', 'our', 'somewhere', 'often', 'from', 'interest', 'while', 'same', 'within', 'amongst', 'whole', 'forty', 'various', 'they', 'alone', 'enough', 'everyone', 'him', 'hereby', 'make', 'you', 'eight', 'take', 'used', 'one', 'system', 'mostly', 'must', 'using', 'nine', 'kg', 'because', 'again', 'un', 'towards', 'her', 'under', 'next', 'ourselves', 'mill', 'say', 'side', 'already', 'among', 'hers', 'thus', 'did', 'for', 'last', 'indeed', 'whereafter', 'but', 'someone', 'since', 're', 'get', 'though', 'off', 'via', 'keep', 'whenever', 'ever', 'of', 'being', 'third', 'between', 'own', 'be', 'afterwards', 'along', 'besides', 'before', 'why', 'fire', 'de', 'seeming', 'beyond', 'six', 'or', 'perhaps', 'is', 'out', 'latter', 'should', 'wherein', 'with', 'therefore', 'co', 'at', 'please', 'are', 'sincere', 'without', 'amoungst', 'during', 'seems', 'hereupon', 'then', 'on', 'whereas', 'beforehand', 'move', 'cant', 'which', 'seem', 'sometimes', 'describe', 'when', 'where', 'latterly', 'down', 'done', 'until', 'itself', 'just', 'else', 'much', 'becoming', 'up', 'am', 'us', 'wherever', 'so', 'whereby', 'not', 'few', 'his', 'moreover', 'becomes', 'this', 'fifteen', 'once', 'found', 'regarding', 'whether', 'do', 'eg', 'its', 'rather', 'than', 'yet', 'sixty', 'were', 'against', 'may', 'throughout', 'whither', 'nowhere', 'four', 'almost', 'onto', 'see', 'thin', 'find', 'toward', 'two', 'together', 'some', 'really', 'around', 'somehow', 'well', 'therein', 'elsewhere', 'sometime', 'detail', 'it', 'herein', 'beside', 'above', 'over', 'has', 'here', 'cannot', 'ours', 'none', 'might', 'i', 'even', 'a', 'thereupon', 'bottom', 'ten', 'the', 'all', 'had', 'whom', 'serious', 'namely', 'thereby', 'put', 'anyway', 'can', 'bill', 'themselves', 'every', 'meanwhile', 'computer', 'as', 'front', 'due', 'through', 'very', 'top', 'no', 'former', 'was', 'below', 'to', 'go', 'became', 'me', 'first', 'fifty', 'there', 'show', 'doesn', 'otherwise', 'become', 'another', 'he', 'yours', 'could', 'nobody', 'we', 'etc', 'cry', 'whose', 'con', 'other', 'she', 'been', 'everywhere', 'each', 'twenty', 'nothing', 'both', 'if', 'full', 'thru', 'have', 'five', 'about', 'didn', 'something'})\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### B-1-a) Adding Stop Words to Default Gensim Stop Words List\nTo access the list of Gensim stop words, you need to import the frozen set **STOPWORDS** from the **gensim.parsing.preprocessong** package. A frozen set in Python is a type of set which is immutable. You cannot add or remove elements in a frozen set. Hence, to add an element, you have to apply the **union** function on the frozen set and pass it the set of new stop words. The union method will return a new set which contains your newly added stop words, as shown below.\n\n*See below code for stop words in Gensim:*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nfrom gensim.parsing.preprocessing import STOPWORDS\n\n# added our own stopword \"sample\" and \"filtration\" in list of pre-defined stopwords\nall_stopwords_gensim = STOPWORDS.union(set(['sample', 'filtration']))\n\n# tokenized\ntext_tokens = word_tokenize(text)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords = [word for word in text_tokens if not word in all_stopwords_gensim]\nprint('Tokens without stopwords : ', tokens_without_stopwords)\n\nprint('\\nOriginal text : ', text)\nfiltered_sentence = (\" \").join(tokens_without_stopwords)\nprint('Filtered text : ', filtered_sentence)","execution_count":8,"outputs":[{"output_type":"stream","text":"\nTokens in text :  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\nTokens without stopwords :  ['This', 'sentence', ',', 'showing', 'stop', 'words', '.']\n\nOriginal text :  This is a sample sentence, showing off the stop words filtration.\nFiltered text :  This sentence , showing stop words .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- From the output above, you can see that the words **sample** and **filtration** have been treated as stop words and consequently have been removed from the input sentence."},{"metadata":{},"cell_type":"markdown","source":"### B-2-a) Remove Stop Words to Default Gensim Stop Words List\nTo remove stop words from Gensim's list of stop words, you have to call the **difference()** method on the frozen set object, which contains the list of stop words. You need to pass a set of stop words that you want to remove from the **frozen set** to the difference() method. The difference() method returns a set which contains all the stop words except those passed to the difference() method.\n\n*The following script removes the word **off** from the set of stop words in Gensim*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nfrom gensim.parsing.preprocessing import STOPWORDS\n\nall_stopwords_gensim = STOPWORDS\n# added our own stopword \"off\"\nsw_list = {\"off\"}\nall_stopwords_gensim = STOPWORDS.difference(sw_list)\n\n# text data\ntext = 'This is a sample sentence, showing off the stop words filtration.'\n\n# tokenized\ntext_tokens = word_tokenize(text)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords = [word for word in text_tokens if not word in all_stopwords_gensim]\nprint('Tokens without stopwords : ', tokens_without_stopwords)\n\nprint('\\nOriginal text : ', text)\nfiltered_sentence = (\" \").join(tokens_without_stopwords)\nprint('Filtered text : ', filtered_sentence)","execution_count":9,"outputs":[{"output_type":"stream","text":"\nTokens in text :  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\nTokens without stopwords :  ['This', 'sample', 'sentence', ',', 'showing', 'off', 'stop', 'words', 'filtration', '.']\n\nOriginal text :  This is a sample sentence, showing off the stop words filtration.\nFiltered text :  This sample sentence , showing off stop words filtration .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- Since the word **off** has now been removed from the stop word set, you can see that it has not been removed from the input sentence after stop word removal."},{"metadata":{},"cell_type":"markdown","source":"## C) Method 3: Using the SpaCy Library\nThe SpaCy library in Python is yet another extremely useful language for natural language processing in Python.\n\n- Need to install **SpaCy library** with **language model** as per business case. Several models exist in SpaCy for different languages. We will be installing the English language model. Execute the following command in your terminal:"},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Spacy installation command\n!pip install -U spacy","execution_count":10,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (2.3.5)\nCollecting spacy\n  Downloading spacy-3.0.3-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n\u001b[K     |████████████████████████████████| 12.7 MB 5.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.0.5)\nRequirement already satisfied: pathy in /opt/conda/lib/python3.7/site-packages (from spacy) (0.3.5)\nCollecting catalogue<2.1.0,>=2.0.1\n  Downloading catalogue-2.0.1-py3-none-any.whl (9.6 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy) (49.6.0.post20201009)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (20.8)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.11.2)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.19.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.1)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.8.2)\nRequirement already satisfied: pydantic<1.8.0,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.7.3)\nCollecting thinc<8.1.0,>=8.0.0\n  Downloading thinc-8.0.1-cp37-cp37m-manylinux2014_x86_64.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 7.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (4.55.1)\nRequirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.3.2)\nCollecting srsly<3.0.0,>=2.4.0\n  Downloading srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n\u001b[K     |████████████████████████████████| 456 kB 7.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.25.1)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.7.4)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->spacy) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy) (2.4.7)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.2)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\nRequirement already satisfied: click<7.2.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy) (1.1.1)\nRequirement already satisfied: smart-open<4.0.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pathy->spacy) (3.0.0)\nInstalling collected packages: catalogue, srsly, thinc, spacy\n  Attempting uninstall: catalogue\n    Found existing installation: catalogue 1.0.0\n    Uninstalling catalogue-1.0.0:\n      Successfully uninstalled catalogue-1.0.0\n  Attempting uninstall: srsly\n    Found existing installation: srsly 1.0.5\n    Uninstalling srsly-1.0.5:\n      Successfully uninstalled srsly-1.0.5\n  Attempting uninstall: thinc\n    Found existing installation: thinc 7.4.5\n    Uninstalling thinc-7.4.5:\n      Successfully uninstalled thinc-7.4.5\n  Attempting uninstall: spacy\n    Found existing installation: spacy 2.3.5\n    Uninstalling spacy-2.3.5:\n      Successfully uninstalled spacy-2.3.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nen-core-web-sm 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.0.3 which is incompatible.\nen-core-web-lg 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.0.3 which is incompatible.\nallennlp 2.0.1 requires spacy<2.4,>=2.1.0, but you have spacy 3.0.3 which is incompatible.\u001b[0m\nSuccessfully installed catalogue-2.0.1 spacy-3.0.3 srsly-2.4.0 thinc-8.0.1\n","name":"stdout"}]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# English language model 'en' installation command\n!python -m spacy download en","execution_count":11,"outputs":[{"output_type":"stream","text":"\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Pleaseuse the\nfull pipeline package name 'en_core_web_sm' instead.\u001b[0m\nCollecting en-core-web-sm==3.0.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n\u001b[K     |████████████████████████████████| 13.7 MB 6.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-sm==3.0.0) (3.0.3)\nRequirement already satisfied: thinc<8.1.0,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.1)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.55.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\nRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.8)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\nRequirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\nRequirement already satisfied: pathy in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.5)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.3.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (49.6.0.post20201009)\nRequirement already satisfied: pydantic<1.8.0,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.2)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\nRequirement already satisfied: click<7.2.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\nRequirement already satisfied: smart-open<4.0.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\nInstalling collected packages: en-core-web-sm\n  Attempting uninstall: en-core-web-sm\n    Found existing installation: en-core-web-sm 2.3.1\n    Uninstalling en-core-web-sm-2.3.1:\n      Successfully uninstalled en-core-web-sm-2.3.1\nSuccessfully installed en-core-web-sm-3.0.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load library\nimport spacy\n\n# load language model\nsp = spacy.load('en_core_web_sm')\n\n# load stop words in Spacy\nall_stopwords = sp.Defaults.stop_words\n\n# text data\ntext = 'This is a sample sentence, showing off the stop words filtration.'\n\n# tokenized\ntext_tokens = word_tokenize(text)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords= [word for word in text_tokens if not word in all_stopwords]\nprint('Tokens without stopwords : ', tokens_without_stopwords)\n\nprint('\\nOriginal text : ', text)\nfiltered_sentence = (\" \").join(tokens_without_stopwords)\nprint('Filtered text : ', filtered_sentence)","execution_count":12,"outputs":[{"output_type":"stream","text":"\nTokens in text :  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\nTokens without stopwords :  ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n\nOriginal text :  This is a sample sentence, showing off the stop words filtration.\nFiltered text :  This sample sentence , showing stop words filtration .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- In the code above we first load the language model and store it in the sp variable. The **sp.Default.stop_words** is a set of default stop words for English language model in SpaCy."},{"metadata":{},"cell_type":"markdown","source":"### C-1)Adding and Removing Stop Words in SpaCy Default Stop Word List\n\n**C-1-a) Adding Stop Words to Default SpaCy Stop Words List**\n\n**C-1-b) Removing Stop Words from Default SpaCy Stop Words List**\n\nLike the other NLP and Gensim, you can also add or remove stop words from the default stop word list in Spacy. Before that, let's look on a list of all the existing stop words in SpaCy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# length of stop words in Spacy\nprint('Length of stop words in Spacy :', len(all_stopwords))\n# stop words in Spacy\nprint('Stop words in Spacy : ', all_stopwords)","execution_count":13,"outputs":[{"output_type":"stream","text":"Length of stop words in Spacy : 326\nStop words in Spacy :  {'into', 'doing', 'most', 'too', 'less', 'call', 'least', 'who', 'twelve', 'part', 'whence', \"'ve\", 'what', 'yourselves', 'hence', 'in', 'and', 'amount', 'eleven', 'by', 'how', 'per', 'yourself', 'never', 'many', 'neither', 'formerly', 'hundred', 'however', 'herself', 'anything', 'name', 'your', 'anywhere', 'anyone', 'others', 'their', 'three', 'such', 'thence', 'them', 'unless', 'several', 'whoever', 'behind', \"n't\", 'whatever', 'still', 'seemed', 'would', 'does', 'only', 'any', 'more', 'made', 'further', 'mine', 'empty', 'give', 'also', 'n‘t', 'everything', 'hereafter', 'upon', 'back', 'except', 'across', 'whereupon', 'always', 'quite', 'now', 'after', 'nor', \"'re\", 'those', 'either', 'will', 'noone', 'thereafter', 'anyhow', 'himself', 'an', 'myself', 'although', 'nevertheless', 'these', 'that', 'my', 'our', '‘ll', 'somewhere', 'often', 'from', 'while', 'same', 'within', 'amongst', 'whole', '’d', 'forty', 'various', 'they', 'alone', 'enough', 'everyone', 'him', 'hereby', 'make', 'you', 'eight', 'take', 'used', 'one', 'mostly', 'must', 'nine', 'using', 'because', 'again', 'her', 'towards', 'under', 'next', 'ourselves', 'say', 'side', 'already', 'among', 'hers', 'thus', 'did', 'for', 'last', 'indeed', 'but', 'whereafter', 'someone', 'since', 're', 'get', 'though', 'off', '‘re', 'via', '‘ve', 'keep', 'whenever', 'ever', 'of', 'being', 'third', 'between', 'own', 'be', 'afterwards', 'along', 'besides', 'before', 'why', 'seeming', 'beyond', 'six', 'or', '’re', 'perhaps', 'is', 'out', '’s', 'latter', 'should', 'wherein', \"'s\", 'with', 'therefore', 'at', 'n’t', 'please', 'are', 'without', 'during', 'seems', 'hereupon', 'then', 'on', 'whereas', 'beforehand', 'move', 'which', 'seem', 'sometimes', 'when', 'where', 'latterly', 'down', 'done', 'until', 'itself', '’ve', 'just', 'else', 'much', 'becoming', 'up', 'am', 'us', 'so', 'wherever', 'whereby', 'not', 'few', 'his', 'moreover', 'becomes', 'this', 'fifteen', 'once', 'regarding', 'whether', 'do', 'its', 'rather', 'than', 'sixty', 'yet', 'were', 'against', 'may', 'throughout', 'whither', 'nowhere', 'four', 'almost', 'onto', 'see', 'toward', 'two', 'together', 'some', 'really', 'around', '’m', '‘m', 'somehow', 'well', 'elsewhere', 'therein', 'sometime', 'herein', 'it', 'beside', 'above', 'over', 'has', 'here', 'cannot', 'ca', 'ours', '‘d', \"'ll\", 'none', 'might', 'i', 'even', 'a', \"'m\", 'thereupon', 'bottom', 'ten', 'the', 'all', 'had', 'serious', 'whom', 'namely', '’ll', 'thereby', 'put', 'anyway', 'can', \"'d\", '‘s', 'themselves', 'every', 'meanwhile', 'as', 'due', 'front', 'through', 'top', 'very', 'no', 'former', 'was', 'below', 'to', 'go', 'became', 'me', 'first', 'fifty', 'there', 'show', 'otherwise', 'become', 'another', 'yours', 'he', 'could', 'nobody', 'we', 'whose', 'other', 'she', 'been', 'everywhere', 'each', 'twenty', 'nothing', 'both', 'if', 'full', 'thru', 'have', 'five', 'about', 'something'}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### C-1-a) Adding Stop Words to Default SpaCy Stop Words List\nThe SpaCy stop word list is basically a set of strings. You can add a new word to the set like you would add any new item to a **set**."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nsp = spacy.load('en_core_web_sm')\n\n# load Spacy stop words and add our own stop word \"sample\"\nall_stopwords = sp.Defaults.stop_words\nall_stopwords.add(\"sample\")\n\n# text data\ntext = 'This is a sample sentence, showing off the stop words filtration.'\n\n# tokenized\ntext_tokens = word_tokenize(text)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords1 = [word for word in text_tokens if not word in all_stopwords]\nprint('Tokens without stopwords : ', tokens_without_stopwords1)\n\nprint('\\nOriginal text : ', text)\nfiltered_sentence = (\" \").join(tokens_without_stopwords1)\nprint('Filtered text : ', filtered_sentence)","execution_count":14,"outputs":[{"output_type":"stream","text":"\nTokens in text :  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\nTokens without stopwords :  ['This', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n\nOriginal text :  This is a sample sentence, showing off the stop words filtration.\nFiltered text :  This sentence , showing stop words filtration .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- The output shows that the word **sample** has been removed from the input sentence.\n\nYou can also add multiple words to the list of stop words in SpaCy as shown below. The following script adds **Kunal** and **Kolhe** to the list of stop words in SpaCy"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nsp = spacy.load('en_core_web_sm')\n\n# load Spacy stop words and add our own stop word \"sample\"\nall_stopwords = sp.Defaults.stop_words\n\nall_stopwords = sp.Defaults.stop_words\nall_stopwords |= {\"Kunal\",\"Kolhe\",}\n\n# text data\ntext_data = 'This is how we are making our processed content more efficient by removing words that do \\\nnot contribute to any future operations. This article is contributed by Kunal Kolhe.'\n\n# tokenized\ntext_tokens = word_tokenize(text_data)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords1 = [word for word in text_tokens if not word in all_stopwords]\nprint('Tokens without stopwords : ', tokens_without_stopwords1)\n\nprint('\\nOriginal text : ', text_data)\nfiltered_sentence = (\" \").join(tokens_without_stopwords1)\nprint('Filtered text : ', filtered_sentence)","execution_count":15,"outputs":[{"output_type":"stream","text":"\nTokens in text :  ['This', 'is', 'how', 'we', 'are', 'making', 'our', 'processed', 'content', 'more', 'efficient', 'by', 'removing', 'words', 'that', 'do', 'not', 'contribute', 'to', 'any', 'future', 'operations', '.', 'This', 'article', 'is', 'contributed', 'by', 'Kunal', 'Kolhe', '.']\nTokens without stopwords :  ['This', 'making', 'processed', 'content', 'efficient', 'removing', 'words', 'contribute', 'future', 'operations', '.', 'This', 'article', 'contributed', '.']\n\nOriginal text :  This is how we are making our processed content more efficient by removing words that do not contribute to any future operations. This article is contributed by Kunal Kolhe.\nFiltered text :  This making processed content efficient removing words contribute future operations . This article contributed .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- The ouput shows tha the words **Kunal** and **Kolhe** both have been removed from the input sentence.\n\n### C-1-b) Removing Stop Words from Default SpaCy Stop Words List\nTo remove a word from the set of stop words in SpaCy, you can pass the word to remove to the **remove** method of the set.\n\nThe following script removes the word not from the set of stop words in SpaCy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nimport spacy\nsp = spacy.load('en_core_web_sm')\n\n# load stop words\nall_stopwords = sp.Defaults.stop_words\nall_stopwords.remove('is')\n\n# text data\ntext = 'This is a sample sentence, showing off the stop words filtration.'\n\n# tokenized\ntext_tokens = word_tokenize(text)\nprint('\\nTokens in text : ', text_tokens)\n\n# applied the tokens in our data\ntokens_without_stopwords = [word for word in text_tokens if not word in all_stopwords]\nprint('Tokens without stopwords : ', tokens_without_stopwords)\n\nprint('\\nOriginal text : ', text)\nfiltered_sentence = (\" \").join(tokens_without_stopwords)\nprint('Filtered text : ', filtered_sentence)","execution_count":16,"outputs":[{"output_type":"stream","text":"\nTokens in text :  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\nTokens without stopwords :  ['This', 'is', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n\nOriginal text :  This is a sample sentence, showing off the stop words filtration.\nFiltered text :  This is sentence , showing stop words filtration .\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- In the output, you can see that the word **is** has not been removed from the input sentence.\n\n## D) Method 4: Using Custom Script to Remove Stop Words\n- If you want full control over stop word removal, you can write your own script to remove stop words from your string.\n\nThe first step in this regard is to define a list of words that you want treated as stop words. Let's create a list of some of the most commonly used stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# stop word list (user defined)\nmy_stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", \n                'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', \n                'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n                'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \n                'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n                'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', \n                'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', \n                'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', \n                'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", \n                'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \n                \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", \n                'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", \n                'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n               ]\n\n# define a function that will accept a string as a parameter and will return the sentence without the stop words\ndef remove_mystopwords_fun(sentence):\n    tokens = sentence.split(\" \")\n    tokens_filtered_a = [word for word in tokens if not word in my_stopwords_list]\n    return (\" \").join(tokens_filtered_a)\n\n# remove stop words from a sample sentence\ntext = \"Today is the 330th day since India implemented a nationwide lockdown to help curb the novel coronavirus pandemic.\\\nIndia's tally of COVID-19 cases rose to 1,09,50,201 with 12,881 new infections being reported in a day,while the recoveries \\\nhave surged to1,06,56,845, according to Union Health Ministry data updated on Thursday. The death toll increased to1,56,014\"\n\nfiltered_text_2 = remove_mystopwords_fun(text)\nprint('Original text : ', text)\nprint('\\nFiltered text : ', filtered_text_2)","execution_count":17,"outputs":[{"output_type":"stream","text":"Original text :  Today is the 330th day since India implemented a nationwide lockdown to help curb the novel coronavirus pandemic.India's tally of COVID-19 cases rose to 1,09,50,201 with 12,881 new infections being reported in a day,while the recoveries have surged to1,06,56,845, according to Union Health Ministry data updated on Thursday. The death toll increased to1,56,014\n\nFiltered text :  Today 330th day since India implemented nationwide lockdown help curb novel coronavirus pandemic.India's tally COVID-19 cases rose 1,09,50,201 12,881 new infections reported day,while recoveries surged to1,06,56,845, according Union Health Ministry data updated Thursday. The death toll increased to1,56,014\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"- Thanks for visiting. if you learned somethings new today, upvote and feel free to comments."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}